{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home1/wangtianshu/universal-blocker\")\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "data_dirs = [\n",
    "    d.name\n",
    "    for d in Path(\"./data/blocking\").iterdir()\n",
    "    if d.name not in [\"songs\", \"citeseer-dblp\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import py_stringmatching as sm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "qgram_tokenizer = sm.tokenizer.qgram_tokenizer.QgramTokenizer(qval=5, padding=False)\n",
    "whitespace_tokenzier = sm.tokenizer.whitespace_tokenizer.WhitespaceTokenizer()\n",
    "cosine = sm.similarity_measure.cosine.Cosine()\n",
    "\n",
    "from src.models import SimCSE\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def sparse_similarity(\n",
    "    s1,\n",
    "    s2,\n",
    "    tokenizer,\n",
    "    similarity,\n",
    "):\n",
    "    t1 = tokenizer.tokenize(s1)\n",
    "    t2 = tokenizer.tokenize(s2)\n",
    "    return similarity.get_sim_score(t1, t2)\n",
    "\n",
    "def ngram_similarity(s1, s2):\n",
    "    return sparse_similarity(s1, s2, tokenizer=qgram_tokenizer, similarity=cosine)\n",
    "\n",
    "def token_similarity(s1, s2):\n",
    "    return sparse_similarity(s1, s2, tokenizer=whitespace_tokenzier, similarity=cosine)\n",
    "\n",
    "def cosine_similarity(e1, e2):\n",
    "    return 1 - distance.cosine(e1, e2)\n",
    "\n",
    "def prepare_model():\n",
    "    global model, tokenizer, device\n",
    "    model_name_or_path=\"./models/roberta-base\"\n",
    "    device=5\n",
    "    model = SimCSE(model_name_or_path=model_name_or_path, max_length=256)\n",
    "    model = model.load_from_checkpoint(\"results/fit/simcse/gittables/12xm9v0r/checkpoints/step=2400-AP=0.45223.ckpt\")\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    tokenizer = model.convert_to_features.keywords[\"tokenizer\"]\n",
    "    \n",
    "prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from random import randrange\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "def get_text(batch):\n",
    "    columns = [c for c in batch.keys() if \"id\" not in c]\n",
    "    batch_size = len(next(iter(batch.values())))\n",
    "\n",
    "    records = []\n",
    "    for i in range(batch_size):\n",
    "        records.append([(c, batch[c][i]) for c in columns])\n",
    "\n",
    "    texts = [\n",
    "        \" \".join(str(t[1]).lower() for t in record if t[1] is not None)\n",
    "        for record in records\n",
    "    ]\n",
    "    features = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    embeddings = F.normalize(model(features)).cpu()\n",
    "\n",
    "    return {\"_text\": texts, \"embeddings\": embeddings}\n",
    "\n",
    "def prepare_pair(matches, dfs):\n",
    "    pairs = pd.merge(matches, dfs[0], left_on=\"id1\", right_on=\"id\")[[\"id1\", \"id2\", \"_text\", \"embeddings\"]]\n",
    "    pairs = pd.merge(pairs, dfs[len(dfs) - 1], left_on=\"id2\", right_on=\"id\")\n",
    "    pairs = pairs[[\"id1\", \"id2\", \"_text_x\", \"_text_y\", \"embeddings_x\", \"embeddings_y\"]]\n",
    "    return pairs\n",
    "\n",
    "def get_similarity(batch):\n",
    "    s1_lst = batch[\"_text_x\"]\n",
    "    s2_lst = batch[\"_text_y\"]\n",
    "    e1_lst = batch[\"embeddings_x\"]\n",
    "    e2_lst = batch[\"embeddings_y\"]\n",
    "    \n",
    "    ngram_sims = [ngram_similarity(s1, s2) for s1, s2 in zip(s1_lst, s2_lst)]\n",
    "    token_sims = [token_similarity(s1, s2) for s1, s2 in zip(s1_lst, s2_lst)]\n",
    "    dense_sims = [cosine_similarity(e1, e2) for e1, e2 in zip(e1_lst, e2_lst)]\n",
    "    \n",
    "    return {\n",
    "        \"ngram\": ngram_sims,\n",
    "        \"token\": token_sims,\n",
    "        \"dense\": dense_sims,\n",
    "    }\n",
    "\n",
    "times = 5\n",
    "for data_dir in sorted(data_dirs):\n",
    "    print(data_dir)\n",
    "    \n",
    "    table_paths = sorted(Path(f\"./data/blocking/{data_dir}\").glob(\"[1-2]*.csv\"))\n",
    "    ds_paths = [Path(f\"./data/blocking_map/{data_dir}/{p.stem}\") for p in table_paths]\n",
    "    \n",
    "    for i, ds_path in enumerate(ds_paths):\n",
    "        if not ds_path.exists():\n",
    "            df = pd.read_csv(table_paths[i], index_col=\"id\", low_memory=False)\n",
    "            ds = Dataset.from_pandas(df)\n",
    "            ds = ds.map(get_text, batched=True, batch_size=32)\n",
    "            ds.save_to_disk(ds_path)\n",
    "    datasets = [Dataset.load_from_disk(p) for p in ds_paths]\n",
    "    \n",
    "    \n",
    "    for ds in datasets:\n",
    "        ds.set_format(\"numpy\")\n",
    "    \n",
    "    matches_path = Path(f\"./data/blocking_map/{data_dir}/matches_{times}\")\n",
    "    if not matches_path.exists():\n",
    "        label_path = Path(f\"./data/blocking/{data_dir}/matches.csv\")\n",
    "        matches = pd.read_csv(label_path)\n",
    "        dfs = [ds.to_pandas() for ds in datasets]\n",
    "        matches = prepare_pair(matches, dfs)\n",
    "        matches = Dataset.from_pandas(matches, preserve_index=False)\n",
    "        matches = matches.map(get_similarity, batched=True, batch_size=32)\n",
    "        matches.save_to_disk(matches_path)\n",
    "    \n",
    "    mismatches_path = Path(f\"./data/blocking_map/{data_dir}/mismatches_{times}\")\n",
    "    if not mismatches_path.exists():\n",
    "        label_path = Path(f\"./data/blocking/{data_dir}/matches.csv\")\n",
    "        matches = pd.read_csv(label_path)\n",
    "        dfs = [ds.to_pandas() for ds in datasets]\n",
    "        mismatches = set()\n",
    "        for k in tqdm(range(len(matches) * times)):\n",
    "            ind1 = randrange(len(dfs[0]))\n",
    "            ind2 = randrange(len(dfs[len(dfs) - 1]))\n",
    "            id1 = dfs[0].iloc[ind1][\"id\"]\n",
    "            id2 = dfs[len(dfs) - 1].iloc[ind2][\"id\"]\n",
    "            mismatches.add((id1, id2))\n",
    "        mismatches = mismatches - set(matches.itertuples(index=False, name=None))\n",
    "        \n",
    "        mismatches = pd.DataFrame(mismatches, columns =[\"id1\", \"id2\"])\n",
    "        mismatches = prepare_pair(mismatches, dfs)\n",
    "        mismatches = Dataset.from_pandas(mismatches, preserve_index=False)\n",
    "        mismatches = mismatches.map(get_similarity, batched=True, batch_size=32)\n",
    "        mismatches.save_to_disk(mismatches_path)\n",
    "        \n",
    "    matches = pd.DataFrame(Dataset.load_from_disk(matches_path))\n",
    "    mismatches = pd.DataFrame(Dataset.load_from_disk(mismatches_path))\n",
    "    \n",
    "#     sorted_matches = {}\n",
    "#     sorted_mismatches = {}\n",
    "#     for t in [\"ngram\", \"dense\"]:\n",
    "#         sorted_matches[t] = sorted(matches[t])\n",
    "#         sorted_mismatches[t] = sorted(mismatches[t])\n",
    "    \n",
    "#     tpr_threshold = {}\n",
    "#     print(f\"TPR={0.9}:\")\n",
    "#     for t in [\"ngram\", \"dense\"]:\n",
    "#         # 1. TPR = 0.9 FDR(FPR)=?\n",
    "#         threshold = sorted_matches[t][math.floor(len(sorted_matches[t]) * 0.1) - 1]\n",
    "#         TP = len(list(filter(lambda x: x >= threshold, sorted_matches[t])))\n",
    "#         FP = len(list(filter(lambda x: x >= threshold, sorted_mismatches[t])))\n",
    "#         FDR = FP / (TP + FP)\n",
    "# #         FPR = FP / len(sorted_mismatches[t])\n",
    "#         print(f\"{t}, FDR={FDR}\")\n",
    "#         tpr_threshold[t] = threshold\n",
    "#     print(tpr_threshold)\n",
    "\n",
    "    \n",
    "#     # 2. FPR = 1~0.9 TPR\n",
    "#     for FPR in range(100, 94, -1):\n",
    "#         FPR = FPR / 100\n",
    "#         print(f\"FPR={FPR}:\")\n",
    "#         for t in [\"ngram\", \"dense\"]:\n",
    "#             threshold = sorted_mismatches[t][math.ceil(len(sorted_mismatches[t]) * FPR) - 1]\n",
    "#             TP = len(list(filter(lambda x: x >= threshold, sorted_matches[t])))\n",
    "#             TPR = TP / len(sorted_matches[t])\n",
    "#             print(f\"{t} TPR={TPR}\")\n",
    "#     print()\n",
    "        \n",
    "#         # 2. FPR = 0.9 FNR=? FN/P = 1 - TP/P\n",
    "#         threshold = sorted_mismatches[math.ceil(len(mismatches) * 0.9) - 1]\n",
    "#         FN = len(list(filter(lambda x: x < threshold, sorted_matches)))\n",
    "#         FNR = FN / len(sorted_matches)\n",
    "#         print(t, threshold, FNR)\n",
    "        \n",
    "#         # 3. FPR = 0 FNR=?\n",
    "#         threshold = sorted_mismatches[math.ceil(len(mismatches) * 1) - 1]\n",
    "#         TP = len(list(filter(lambda x: x >= threshold, sorted_matches)))\n",
    "#         TPR = TP / len(sorted_matches)\n",
    "#         print(t, threshold, TPR)\n",
    "        \n",
    "    matches[\"label\"] = \"matched\"\n",
    "    mismatches[\"label\"] = \"mismatched\"\n",
    "    \n",
    "    df = pd.concat([matches, mismatches])[[\"label\", \"ngram\", \"token\", \"dense\"]]\n",
    "    flatten = []\n",
    "    for item in df.to_dict('records'):\n",
    "        for t in [\"ngram\", \"token\", \"dense\"]:\n",
    "            flatten.append({\"label\": item[\"label\"], \"type\": t, \"similarity\": item[t]})\n",
    "    df_flatten = pd.DataFrame(flatten)\n",
    "    \n",
    "    sns.displot(data=df_flatten[df_flatten[\"type\"] != \"token\"], x=\"similarity\", hue=\"label\", col=\"type\", stat=\"density\", bins=20, kde=True, multiple=\"stack\")\n",
    "    sns.jointplot(data=df, x=\"dense\", y=\"ngram\", hue=\"label\")\n",
    "    \n",
    "    plt.show()\n",
    "#     for t in [\"ngram\", \"token\", \"dense\"]:\n",
    "#         max_mismatches = max(mismatches[t])\n",
    "#         area1 = len(list(filter(lambda x: x > max_mismatches, matches[t]))) / len(matches)\n",
    "    \n",
    "#     for t in [\"ngram\", \"token\", \"dense\"]:\n",
    "#         sns.diplot(matches[t])\n",
    "#         plt.hist(matches[t], bins=20, alpha=0.5, label=\"matches\", density=True, stacked=True)\n",
    "#         plt.axvline(mean(matches[t]), color=\"k\", alpha=0.5, linestyle='dashed')\n",
    "#         plt.hist(mismatches[t], bins=20, alpha=0.5, label=\"mismatches\", density=True, stacked=True)\n",
    "#         plt.axvline(mean(mismatches[t]), color=\"k\", alpha=0.5, linestyle='dashed')\n",
    "#         plt.legend(loc='upper right')\n",
    "#         plt.title(f\"{data_dir}-{t}\")\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b53356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from random import randrange\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "def get_text(batch):\n",
    "    columns = [c for c in batch.keys() if \"id\" not in c]\n",
    "    batch_size = len(next(iter(batch.values())))\n",
    "\n",
    "    records = []\n",
    "    for i in range(batch_size):\n",
    "        records.append([(c, batch[c][i]) for c in columns])\n",
    "\n",
    "    texts = [\n",
    "        \" \".join(str(t[1]).lower() for t in record if t[1] is not None)\n",
    "        for record in records\n",
    "    ]\n",
    "    features = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    embeddings = F.normalize(model(features)).cpu()\n",
    "\n",
    "    return {\"_text\": texts, \"embeddings\": embeddings}\n",
    "\n",
    "def prepare_pair(matches, dfs):\n",
    "    pairs = pd.merge(matches, dfs[0], left_on=\"id1\", right_on=\"id\")[[\"id1\", \"id2\", \"_text\", \"embeddings\"]]\n",
    "    pairs = pd.merge(pairs, dfs[len(dfs) - 1], left_on=\"id2\", right_on=\"id\")\n",
    "    pairs = pairs[[\"id1\", \"id2\", \"_text_x\", \"_text_y\", \"embeddings_x\", \"embeddings_y\"]]\n",
    "    return pairs\n",
    "\n",
    "def get_similarity(batch):\n",
    "    s1_lst = batch[\"_text_x\"]\n",
    "    s2_lst = batch[\"_text_y\"]\n",
    "    e1_lst = batch[\"embeddings_x\"]\n",
    "    e2_lst = batch[\"embeddings_y\"]\n",
    "    \n",
    "    ngram_sims = [ngram_similarity(s1, s2) for s1, s2 in zip(s1_lst, s2_lst)]\n",
    "    token_sims = [token_similarity(s1, s2) for s1, s2 in zip(s1_lst, s2_lst)]\n",
    "    dense_sims = [cosine_similarity(e1, e2) for e1, e2 in zip(e1_lst, e2_lst)]\n",
    "    \n",
    "    return {\n",
    "        \"ngram\": ngram_sims,\n",
    "        \"token\": token_sims,\n",
    "        \"dense\": dense_sims,\n",
    "    }\n",
    "\n",
    "times = 5\n",
    "for data_dir in [\"movies\"]:\n",
    "    print(data_dir)\n",
    "    \n",
    "    table_paths = sorted(Path(f\"./data/blocking/{data_dir}\").glob(\"[1-2]*.csv\"))\n",
    "    ds_paths = [Path(f\"./data/blocking_map/{data_dir}/{p.stem}\") for p in table_paths]\n",
    "    \n",
    "    for i, ds_path in enumerate(ds_paths):\n",
    "        if not ds_path.exists():\n",
    "            df = pd.read_csv(table_paths[i], index_col=\"id\", low_memory=False)\n",
    "            ds = Dataset.from_pandas(df)\n",
    "            ds = ds.map(get_text, batched=True, batch_size=32)\n",
    "            ds.save_to_disk(ds_path)\n",
    "    datasets = [Dataset.load_from_disk(p) for p in ds_paths]\n",
    "    \n",
    "    for ds in datasets:\n",
    "        ds.set_format(\"numpy\")\n",
    "    \n",
    "    matches_path = Path(f\"./data/blocking_map/{data_dir}/matches_{times}\")\n",
    "    if not matches_path.exists():\n",
    "        label_path = Path(f\"./data/blocking/{data_dir}/matches.csv\")\n",
    "        matches = pd.read_csv(label_path)\n",
    "        dfs = [ds.to_pandas() for ds in datasets]\n",
    "        matches = prepare_pair(matches, dfs)\n",
    "        matches = Dataset.from_pandas(matches, preserve_index=False)\n",
    "        matches = matches.map(get_similarity, batched=True, batch_size=32)\n",
    "        matches.save_to_disk(matches_path)\n",
    "    \n",
    "    mismatches_path = Path(f\"./data/blocking_map/{data_dir}/mismatches_{times}\")\n",
    "    if not mismatches_path.exists():\n",
    "        label_path = Path(f\"./data/blocking/{data_dir}/matches.csv\")\n",
    "        matches = pd.read_csv(label_path)\n",
    "        dfs = [ds.to_pandas() for ds in datasets]\n",
    "        mismatches = set()\n",
    "        for k in tqdm(range(len(matches) * times)):\n",
    "            ind1 = randrange(len(dfs[0]))\n",
    "            ind2 = randrange(len(dfs[len(dfs) - 1]))\n",
    "            id1 = dfs[0].iloc[ind1][\"id\"]\n",
    "            id2 = dfs[len(dfs) - 1].iloc[ind2][\"id\"]\n",
    "            mismatches.add((id1, id2))\n",
    "        mismatches = mismatches - set(matches.itertuples(index=False, name=None))\n",
    "        \n",
    "        mismatches = pd.DataFrame(mismatches, columns =[\"id1\", \"id2\"])\n",
    "        mismatches = prepare_pair(mismatches, dfs)\n",
    "        mismatches = Dataset.from_pandas(mismatches, preserve_index=False)\n",
    "        mismatches = mismatches.map(get_similarity, batched=True, batch_size=32)\n",
    "        mismatches.save_to_disk(mismatches_path)\n",
    "        \n",
    "    matches = pd.DataFrame(Dataset.load_from_disk(matches_path))\n",
    "    mismatches = pd.DataFrame(Dataset.load_from_disk(mismatches_path))\n",
    "    print(len(matches))\n",
    "    print(len(mismatches))\n",
    "    \n",
    "    sorted_matches = {}\n",
    "    sorted_mismatches = {}\n",
    "    for t in [\"ngram\", \"dense\"]:\n",
    "        sorted_matches[t] = sorted(matches[t])\n",
    "        sorted_mismatches[t] = sorted(mismatches[t])\n",
    "\n",
    "    # 2. FPR = 1~0.9 TPR\n",
    "    for FPR in range(100, 94, -1):\n",
    "        FPR = FPR / 100\n",
    "        print(f\"FPR={FPR}:\")\n",
    "        for t in [\"ngram\", \"dense\"]:\n",
    "            threshold = sorted_mismatches[t][math.ceil(len(sorted_mismatches[t]) * FPR) - 1]\n",
    "            TP = len(list(filter(lambda x: x >= threshold, sorted_matches[t])))\n",
    "            TPR = TP / len(sorted_matches[t])\n",
    "            print(f\"{t} TPR={TPR}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9d135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
