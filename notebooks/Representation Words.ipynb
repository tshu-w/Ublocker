{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05498957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(\"/home1/wangtianshu/universal-blocker\")\n",
    "data_dirs = [\n",
    "    d.name\n",
    "    for d in Path(\"./data/blocking\").iterdir()\n",
    "    if d.name not in [\"songs\", \"citeseer-dblp\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from src.models import SimCSE\n",
    "\n",
    "device=5\n",
    "model_name_or_path = \"./models/roberta-base/\"\n",
    "simcse = SimCSE(model_name_or_path=model_name_or_path, max_length=256)\n",
    "simcse = simcse.load_from_checkpoint(\"results/fit/simcse/gittables/1cwvyg3q/checkpoints/step=1500-AP=0.46677.ckpt\")\n",
    "simcse.eval()\n",
    "tokenizer = simcse.collate_fn.tokenizer\n",
    "trained = simcse.model\n",
    "trained = trained.to(device)\n",
    "\n",
    "roberta = AutoModel.from_pretrained(model_name_or_path)\n",
    "roberta.eval()\n",
    "roberta = roberta.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import py_stringmatching as sm\n",
    "from datasets import Dataset\n",
    "from src.datamodules.blocking import dict2tuples\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "\n",
    "cosine = sm.similarity_measure.cosine.Cosine()\n",
    "qgram_tokenizer = sm.tokenizer.qgram_tokenizer.QgramTokenizer(qval=5, padding=False)\n",
    "whitespace_tokenizer = sm.tokenizer.whitespace_tokenizer.WhitespaceTokenizer()\n",
    "\n",
    "def encode(batch, model):\n",
    "    collate_fn = getattr(model, \"collate_fn\", None)\n",
    "\n",
    "    batch: list[dict] = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    batch = [dict2tuples(r, \"id\") for r in batch]\n",
    "    texts = [\" \".join([t[1] for t in l]) for l in batch]\n",
    "\n",
    "    batch = move_data_to_device(collate_fn(batch), model.device)\n",
    "    embeddings = F.normalize(model(batch).detach()).to(\"cpu\").numpy()\n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"embeddings\": embeddings,\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_representation_words(model, s):\n",
    "    inputs = tokenizer(\n",
    "        s,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = move_data_to_device(inputs, model.device)\n",
    "    outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "    attentions = outputs.attentions\n",
    "    hidden_states = outputs.hidden_states\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    last_attentions = outputs.attentions[-1]\n",
    "    attention_heads = last_attentions.shape[1]\n",
    "    weight = torch.zeros(last_attentions.shape[2], device=model.device)\n",
    "    for i in range(attention_heads):\n",
    "        weight += last_attentions[0, i].sum(dim=0)\n",
    "    weight /= attention_heads\n",
    "    weight = F.softmax(weight, dim=-1)\n",
    "#     print(weight)\n",
    "    indices = torch.argsort(weight, descending=True)\n",
    "    representation_words = tokenizer.convert_ids_to_tokens(inputs.input_ids[:, indices][0].tolist(), skip_special_tokens=True)\n",
    "    return representation_words[:15]\n",
    "\n",
    "def check_pair(r1, r2, model):\n",
    "    t1 = dict2tuples(r1)\n",
    "    t2 = dict2tuples(r2)\n",
    "    s1 = \" \".join([t[1] for t in t1])\n",
    "    s2 = \" \".join([t[1] for t in t2])\n",
    "#     print(s1, s2, sep=\"\\n\")\n",
    "    rw1 = find_representation_words(model, s1)\n",
    "    rw2 = find_representation_words(model, s2)\n",
    "#     print(rw1, rw2, sep=\"\\n\")\n",
    "#     print()\n",
    "    tw1 = whitespace_tokenzier.tokenize(s1)\n",
    "    tw2 = whitespace_tokenzier.tokenize(s2)\n",
    "    \n",
    "    score1 = cosine.get_sim_score(rw1, rw2)\n",
    "    score2 = cosine.get_sim_score(tw1, tw2)\n",
    "    return score1, score2\n",
    "    \n",
    "for data_dir in [\"imdb-dbpedia\", \"movies\", \"amazon-google\", \"walmart-amazon_homo\", \"walmart-amazon_heter\"]:\n",
    "    print(data_dir)\n",
    "    \n",
    "    table_paths = sorted(Path(f\"./data/blocking/{data_dir}\").glob(\"[1-2]*.csv\"))\n",
    "    dfs = [pd.read_csv(p, index_col=\"id\", low_memory=False) for p in table_paths]\n",
    "    for i in range(len(dfs)):\n",
    "        dfs[i] = dfs[i].fillna(\"\")\n",
    "    \n",
    "    matches_path = Path(f\"./data/blocking/{data_dir}/matches.csv\")\n",
    "    matches = set(\n",
    "        pd.read_csv(matches_path).itertuples(index=False, name=None)\n",
    "    )\n",
    "    for encoder in [trained, roberta]:\n",
    "        pairs = random.sample(list(matches), 100)\n",
    "        score1, score2 = 0, 0\n",
    "        win = 0\n",
    "        for p in pairs:\n",
    "            r1 = dfs[0].loc[p[0]]\n",
    "            r2 = dfs[len(dfs) - 1].loc[p[1]]\n",
    "            s1, s2 = check_pair(r1, r2, encoder)\n",
    "            score1 += s1\n",
    "            score2 += s2\n",
    "            win += s1 > s2\n",
    "            \n",
    "        print(score1, score2, win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a23f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
