{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home1/wangtianshu/universal-blocker\")\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d09f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets.load import Dataset, load_dataset\n",
    "from datasets.features import Array2D, Sequence, Value, Features\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/roberta-base/\")\n",
    "model = AutoModel.from_pretrained(\"./models/roberta-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b708a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def preprocess(batch: dict[list], index_col: str = \"id\", tokenizer=tokenizer, model=model):\n",
    "    columns = [c for c in batch.keys() if index_col not in c]\n",
    "    batch_size = len(next(iter(batch.values())))\n",
    "\n",
    "    records = []\n",
    "    for i in range(batch_size):\n",
    "        records.append([(c, batch[c][i]) for c in columns])\n",
    "    \n",
    "    texts = [\n",
    "        \" \".join(str(t[1]).lower() for t in record if t[1] is not None)\n",
    "        for record in records\n",
    "    ]\n",
    "    features = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs_embeds = model.embeddings.word_embeddings(features[\"input_ids\"])\n",
    "    \n",
    "    return {\"inputs_embeds\": inputs_embeds.numpy()}\n",
    "\n",
    "\n",
    "data_dir = \"./data/blocking/cora\"\n",
    "table_paths = sorted(Path(data_dir).glob(\"[1-2]*.csv\"))\n",
    "datasets = [load_dataset(\"csv\", data_files=str(t), split=\"train\") for t in table_paths]\n",
    "for i, dataset in enumerate(datasets):\n",
    "    datasets[i] = dataset.map(\n",
    "        preprocess,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "        features=Features({\n",
    "            \"inputs_embeds\": Array2D(\n",
    "                shape=(256, model.embeddings.word_embeddings.embedding_dim), dtype=\"float32\"\n",
    "            )\n",
    "        }),\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "def encoder(batch):\n",
    "    \"empty encode function\"\n",
    "    return {}\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    datasets[i] = dataset.map(\n",
    "        encoder,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4bfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def transform(batch: dict[list], index_col: str = \"id\", tokenizer=tokenizer, model=model):\n",
    "    columns = [c for c in batch.keys() if index_col not in c]\n",
    "    batch_size = len(next(iter(batch.values())))\n",
    "\n",
    "    records = []\n",
    "    for i in range(batch_size):\n",
    "        records.append([(c, batch[c][i]) for c in columns])\n",
    "    \n",
    "    texts = [\n",
    "        \" \".join(str(t[1]).lower() for t in record if t[1] is not None)\n",
    "        for record in records\n",
    "    ]\n",
    "    features = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs_embeds = model.embeddings.word_embeddings(features[\"input_ids\"])\n",
    "    \n",
    "    return {\"inputs_embeds\": inputs_embeds.numpy()}\n",
    "\n",
    "datasets = [load_dataset(\"csv\", data_files=str(t), split=\"train\") for t in table_paths]\n",
    "for i, dataset in enumerate(datasets):\n",
    "    datasets[i] = dataset.with_transform(\n",
    "        transform,\n",
    "#         features=Features({\n",
    "#             \"inputs_embeds\": Array2D(\n",
    "#                 shape=(256, model.embeddings.word_embeddings.embedding_dim), dtype=\"float32\"\n",
    "#             )\n",
    "#         }),\n",
    "#         remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "def encoder(batch):\n",
    "    \"empty encode function\"\n",
    "    return {}\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    datasets[i] = dataset.map(\n",
    "        encoder,\n",
    "        batched=True,\n",
    "        batch_size=32,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bba4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
