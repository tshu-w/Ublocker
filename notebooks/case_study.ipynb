{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import auc\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir(\"/home1/wangtianshu/universal-blocker\")\n",
    "data_dirs = [\n",
    "    d.name\n",
    "    for d in Path(\"./data/blocking\").iterdir()\n",
    "    if d.name not in [\"songs\", \"citeseer-dblp\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0459b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import py_stringmatching as sm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "qgram_tokenizer = sm.tokenizer.qgram_tokenizer.QgramTokenizer(qval=5, padding=False)\n",
    "whitespace_tokenzier = sm.tokenizer.whitespace_tokenizer.WhitespaceTokenizer()\n",
    "cosine = sm.similarity_measure.cosine.Cosine()\n",
    "\n",
    "from src.models import SimCSE\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def sparse_similarity(\n",
    "    s1,\n",
    "    s2,\n",
    "    tokenizer,\n",
    "    similarity,\n",
    "):\n",
    "    t1 = tokenizer.tokenize(s1)\n",
    "    t2 = tokenizer.tokenize(s2)\n",
    "    return similarity.get_sim_score(t1, t2)\n",
    "\n",
    "def ngram_similarity(s1, s2):\n",
    "    return sparse_similarity(s1, s2, tokenizer=qgram_tokenizer, similarity=cosine)\n",
    "\n",
    "def token_similarity(s1, s2):\n",
    "    return sparse_similarity(s1, s2, tokenizer=whitespace_tokenzier, similarity=cosine)\n",
    "\n",
    "def cosine_similarity(e1, e2):\n",
    "    return 1 - distance.cosine(e1, e2)\n",
    "\n",
    "def prepare_model():\n",
    "    global model, tokenizer, device\n",
    "    model_name_or_path=\"./models/roberta-base\"\n",
    "    device=5\n",
    "    model = SimCSE(model_name_or_path=model_name_or_path, max_length=256)\n",
    "    model = model.load_from_checkpoint(\"results/fit/simcse/gittables/1cwvyg3q/checkpoints/step=1500-AP=0.46677.ckpt\")\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    tokenizer = model.collate_fn.tokenizer\n",
    "    \n",
    "prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43142c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "from src.datamodules.blocking import dict2tuples\n",
    "from pytorch_lightning.utilities import move_data_to_device\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "def encode(batch):\n",
    "    collate_fn = getattr(model, \"collate_fn\", None)\n",
    "\n",
    "    batch: list[dict] = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    batch = [dict2tuples(r, \"id\") for r in batch]\n",
    "    texts = [\" \".join([t[1] for t in l]) for l in batch]\n",
    "\n",
    "    batch = move_data_to_device(collate_fn(batch), model.device)\n",
    "    embeddings = F.normalize(model(batch).detach()).to(\"cpu\").numpy()\n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"embeddings\": embeddings,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_similarity(r1, r2):\n",
    "    s1 = r1[\"text\"]\n",
    "    s2 = r2[\"text\"]\n",
    "    e1 = r1[\"embeddings\"]\n",
    "    e2 = r2[\"embeddings\"]\n",
    "    \n",
    "    ngram_sim = ngram_similarity(s1, s2)\n",
    "    cosine_sim = cosine_similarity(e1, e2)\n",
    "    \n",
    "    return ngram_sim, cosine_sim\n",
    "\n",
    "def check(p, dfs):\n",
    "    df1, df2 = dfs[0], dfs[len(dfs) - 1]\n",
    "    r1 = df1.loc[p[0]]\n",
    "    r2 = df2.loc[p[1]]\n",
    "    \n",
    "    ngram_sim, cosine_sim = get_similarity(r1, r2)\n",
    "    \n",
    "    print(p)\n",
    "    print(repr(r1[\"text\"]))\n",
    "    print(repr(r2[\"text\"]))\n",
    "    print(f\"spase similarity {ngram_sim}\")\n",
    "    print(f\"dense similarity {cosine_sim}\")\n",
    "    \n",
    "def check_set(st, dfs, num=5):\n",
    "    for p in random.sample(list(st), num):\n",
    "        check(p, dfs)\n",
    "        print()\n",
    "\n",
    "K = 20\n",
    "# for data_dir in data_dirs:\n",
    "# for data_dir in [\"walmart-amazon_homo\", \"imdb-dbpedia\", \"amazon-google\", \"walmart-amazon_heter\", \"movies\"]:\n",
    "for data_dir in [\"imdb-dbpedia\", \"movies\", \"amazon-google\", \"walmart-amazon_homo\", \"walmart-amazon_heter\"]:\n",
    "# for data_dir in [\"imdb-dbpedia\"]:\n",
    "    print(data_dir)\n",
    "    \n",
    "    table_paths = sorted(Path(f\"./data/blocking/{data_dir}\").glob(\"[1-2]*.csv\"))\n",
    "    ds_paths = [Path(f\"./data/blocking_map/{data_dir}/{p.stem}\") for p in table_paths]\n",
    "    \n",
    "    for i, ds_path in enumerate(ds_paths):\n",
    "        if not ds_path.exists():\n",
    "            df = pd.read_csv(table_paths[i], low_memory=False)\n",
    "            df = df.fillna(\"\")\n",
    "            columns = list(df.columns)\n",
    "            columns.remove(\"id\")\n",
    "            df[columns] = df[columns].astype(str)\n",
    "            ds = Dataset.from_pandas(df)\n",
    "            ds = ds.map(encode, batched=True, batch_size=32)\n",
    "            ds.save_to_disk(ds_path)\n",
    "    dfs = [Dataset.load_from_disk(p).to_pandas().set_index('id') for p in ds_paths]\n",
    "    \n",
    "    with Path(f\"./results/debug/sparse_join/{data_dir}.pickle\").open(\"rb\") as f:\n",
    "        sparse_candidates = pickle.load(f)\n",
    "\n",
    "    with Path(f\"./results/debug/simcse/{data_dir}.pickle\").open(\"rb\") as f:\n",
    "        dense_candidates = pickle.load(f)\n",
    "\n",
    "    matches_path = Path(f\"./data/blocking/{data_dir}/matches.csv\")\n",
    "    matches = set(\n",
    "        pd.read_csv(matches_path).itertuples(index=False, name=None)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    sparse_cands = set().union(*[s for s in sparse_candidates[:K]])\n",
    "    dense_cands = set().union(*[s for s in dense_candidates[:K]])\n",
    "    sparse_hits = matches & sparse_cands\n",
    "    dense_hits = matches & dense_cands\n",
    "    sparse_misses = matches - sparse_hits\n",
    "    dense_missses = matches - dense_hits\n",
    "    \n",
    "#     print(len(sparse_hits))\n",
    "#     print(len(dense_hits))\n",
    "#     print(len(sparse_misses))\n",
    "#     print(len(dense_missses))\n",
    "    \n",
    "    both_hits = sparse_hits & dense_hits\n",
    "    sparse_hits_dense_misses = sparse_hits & dense_missses\n",
    "    sparse_misses_dense_hits = sparse_misses & dense_hits\n",
    "    both_misses = sparse_misses & dense_missses\n",
    "    \n",
    "#     samples = []\n",
    "#     df1, df2 = dfs[0], dfs[len(dfs) - 1]\n",
    "    \n",
    "#     for t, s in zip(\n",
    "#         [\"both\", \"sparse\", \"dense\", \"neither\"], \n",
    "#         [both_hits, sparse_hits_dense_misses, sparse_misses_dense_hits, both_misses],\n",
    "#     ):\n",
    "#         for p in s:\n",
    "#             r1 = df1[df1[\"id\"] == p[0]].iloc[0]\n",
    "#             r2 = df2[df2[\"id\"] == p[1]].iloc[0]\n",
    "#             ngram_sim, cosine_sim = get_similarity(r1, r2)\n",
    "        \n",
    "#             instance = {\n",
    "#                 \"pair\": p,\n",
    "#                 \"r1\": r1[\"_text\"],\n",
    "#                 \"r2\": r2[\"_text\"],\n",
    "#                 \"sparse_sim\": ngram_sim,\n",
    "#                 \"dense_sim\": cosine_sim,\n",
    "#                 \"type\": t,\n",
    "#             }\n",
    "#             samples.append(instance)\n",
    "            \n",
    "#     df = pd.DataFrame(samples)\n",
    "#     g = sns.catplot(data=df, x=\"sparse_sim\", y=\"type\", kind=\"violin\", height=3, aspect=2)\n",
    "#     plt.show()\n",
    "    \n",
    "    print(\"---------------------------\")\n",
    "#     print(f\"both hits {len(both_hits)}\")\n",
    "#     check_set(both_hits, dfs)\n",
    "    \n",
    "    print(f\"sparse hits dense misses {len(sparse_hits_dense_misses)}\")\n",
    "    check_set(sparse_hits_dense_misses, dfs, num=5)\n",
    "    \n",
    "    print(\"---------------------------\")\n",
    "    \n",
    "    print(f\"sparse misses dense hits {len(sparse_misses_dense_hits)}\")\n",
    "    check_set(sparse_misses_dense_hits, dfs, num=5)\n",
    "    \n",
    "#     print(f\"both misses {len(both_misses)}\")\n",
    "#     check_set(both_misses, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed8216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "def encode(batch):\n",
    "    collate_fn = getattr(model, \"collate_fn\", None)\n",
    "\n",
    "    batch: list[dict] = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    batch = [dict2tuples(r, \"id\") for r in batch]\n",
    "    texts = [\" \".join([t[1] for t in l]) for l in batch]\n",
    "    texts = [\n",
    "        \" \".join(t[1] for t in l if len(tokenizer.tokenize(t[1])) <= 20)\n",
    "        for l in batch\n",
    "    ]\n",
    "\n",
    "    batch = move_data_to_device(collate_fn(batch), model.device)\n",
    "\n",
    "    embeddings = model(batch).detach().to(\"cpu\").numpy()\n",
    "    embeddings = F.normalize(model(batch).detach()).to(\"cpu\").numpy()\n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "        \"embeddings\": embeddings,\n",
    "    }\n",
    "\n",
    "def get_similarity(r1, r2):\n",
    "    s1 = r1[\"text\"]\n",
    "    s2 = r2[\"text\"]\n",
    "    e1 = r1[\"embeddings\"]\n",
    "    e2 = r2[\"embeddings\"]\n",
    "    \n",
    "    ngram_sim = ngram_similarity(s1, s2)\n",
    "    cosine_sim = cosine_similarity(e1, e2)\n",
    "    \n",
    "    return ngram_sim, cosine_sim\n",
    "\n",
    "\n",
    "def check(p, dfs):\n",
    "    df1, df2 = dfs[0], dfs[len(dfs) - 1]\n",
    "    r1 = df1[df1[\"id\"] == p[0]].iloc[0]\n",
    "    r2 = df2[df2[\"id\"] == p[1]].iloc[0]\n",
    "    \n",
    "    ngram_sim, cosine_sim = get_similarity(r1, r2)\n",
    "    \n",
    "    print(p)\n",
    "    print(r1[\"text\"])\n",
    "    print(r2[\"text\"])\n",
    "    print(f\"spase similarity {ngram_sim}\")\n",
    "    print(f\"dense similarity {cosine_sim}\")\n",
    "    \n",
    "def check_set(st, dfs, num=5):\n",
    "    for p in random.sample(list(st), num):\n",
    "        check(p, dfs)\n",
    "        print()\n",
    "\n",
    "K = 20\n",
    "for data_dir in data_dirs:\n",
    "# for data_dir in [\"walmart-amazon_homo\", \"imdb-dbpedia\", \"amazon-google\", \"walmart-amazon_heter\", \"movies\"]:\n",
    "# for data_dir in [\"amazon-google\"]:\n",
    "    print(data_dir)\n",
    "    \n",
    "    table_paths = sorted(Path(f\"./data/blocking/{data_dir}\").glob(\"[1-2]*.csv\"))\n",
    "    ds_paths = [Path(f\"./data/blocking_map/{data_dir}/{p.stem}\") for p in table_paths]\n",
    "    \n",
    "    for i, ds_path in enumerate(ds_paths):\n",
    "        if not ds_path.exists():\n",
    "            df = pd.read_csv(table_paths[i], index_col=\"id\", low_memory=False)\n",
    "            ds = Dataset.from_pandas(df)\n",
    "            ds = ds.map(encode, batched=True, batch_size=32)\n",
    "            ds.save_to_disk(ds_path)\n",
    "    dfs = [Dataset.load_from_disk(p).to_pandas() for p in ds_paths]\n",
    "    \n",
    "    with Path(f\"./results/debug/sparse_join/{data_dir}.pickle\").open(\"rb\") as f:\n",
    "        sparse_candidates = pickle.load(f)\n",
    "\n",
    "    with Path(f\"./results/debug/simcse/{data_dir}.pickle\").open(\"rb\") as f:\n",
    "        dense_candidates = pickle.load(f)\n",
    "\n",
    "    matches_path = Path(f\"./data/blocking/{data_dir}/matches.csv\")\n",
    "    matches = set(pd.read_csv(matches_path).itertuples(index=False, name=None))\n",
    "    \n",
    "    sparse_cands = set().union(*[s for s in sparse_candidates[:K]])\n",
    "    dense_cands = set().union(*[s for s in dense_candidates[:K]])\n",
    "    sparse_hits = matches & sparse_cands\n",
    "    dense_hits = matches & dense_cands\n",
    "    sparse_misses = matches - sparse_hits\n",
    "    dense_missses = matches - dense_hits\n",
    "    \n",
    "#     print(len(sparse_hits))\n",
    "#     print(len(dense_hits))\n",
    "#     print(len(sparse_misses))\n",
    "#     print(len(dense_missses))\n",
    "    \n",
    "    both_hits = sparse_hits & dense_hits\n",
    "    sparse_hits_dense_misses = sparse_hits & dense_missses\n",
    "    sparse_misses_dense_hits = sparse_misses & dense_hits\n",
    "    both_misses = sparse_misses & dense_missses\n",
    "    \n",
    "    samples = []\n",
    "    df1, df2 = dfs[0], dfs[len(dfs) - 1]\n",
    "    \n",
    "    for t, s in zip(\n",
    "        [\"both\", \"sparse\", \"dense\", \"neither\"], \n",
    "        [both_hits, sparse_hits_dense_misses, sparse_misses_dense_hits, both_misses],\n",
    "    ):\n",
    "        for p in s:\n",
    "            r1 = df1[df1[\"id\"] == p[0]].iloc[0]\n",
    "            r2 = df2[df2[\"id\"] == p[1]].iloc[0]\n",
    "            ngram_sim, cosine_sim = get_similarity(r1, r2)\n",
    "        \n",
    "            instance = {\n",
    "                \"pair\": p,\n",
    "                \"r1\": r1[\"text\"],\n",
    "                \"r2\": r2[\"text\"],\n",
    "                \"sparse_sim\": ngram_sim,\n",
    "                \"dense_sim\": cosine_sim,\n",
    "                \"type\": t,\n",
    "            }\n",
    "            samples.append(instance)\n",
    "            \n",
    "    df = pd.DataFrame(samples)\n",
    "    g = sns.catplot(data=df, x=\"sparse_sim\", y=\"type\", kind=\"violin\", height=3, aspect=2)\n",
    "    plt.show()\n",
    "    \n",
    "#     print(\"---------------------------\")\n",
    "# #     print(f\"both hits {len(both_hits)}\")\n",
    "# #     check_set(both_hits, dfs)\n",
    "    \n",
    "#     print(f\"sparse hits dense misses {len(sparse_hits_dense_misses)}\")\n",
    "#     check_set(sparse_hits_dense_misses, dfs)\n",
    "    \n",
    "#     print(\"---------------------------\")\n",
    "    \n",
    "#     print(f\"sparse misses dense hits {len(sparse_misses_dense_hits)}\")\n",
    "#     check_set(sparse_misses_dense_hits, dfs)\n",
    "    \n",
    "# #     print(f\"both misses {len(both_misses)}\")\n",
    "# #     check_set(both_misses, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def get_embeddings(t):\n",
    "    features = tokenizer(\n",
    "        t,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(5)\n",
    "    embeddings = F.normalize(model(features)).cpu()\n",
    "    return embeddings[0]\n",
    "\n",
    "s1 = \"toshiba transmemory u202, 128 gb 28.99 toshiba 128 gb\"\n",
    "s2 = \"pendrive 128gb toshiba transmemory white, usb 2.0, blanco toshiba 128 g\"\n",
    "e1 = get_embeddings(s1)\n",
    "e2 = get_embeddings(s2)\n",
    "print(cosine_similarity(e1, e2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c500a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
